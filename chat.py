import streamlit as st
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_openai import ChatOpenAI
from langchain import hub
# query -> ì§ì¥ì¸ -> ê±°ì£¼ìë¡œ ë°”ê¾¸ëŠ” chainì„ ì¶”ê°€ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from pinecone import Pinecone


st.set_page_config(page_title="Tax-Chatbot", page_icon="ğŸ“œ")

st.title("ğŸ“œ ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ ê´€ë ¨ ì§ˆë¬¸ì„ ë‚´ìš©ì„ ì…ë ¥í•˜ë©´ ìë™ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•´ì¤ë‹ˆë‹¤.")

load_dotenv()

if "message_list" not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])


# í•¨ìˆ˜ ì´ë¦„ : get_ai_message
# í•¨ìˆ˜ ê¸°ëŠ¥ : user_messageë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ai_messageë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
# í•¨ìˆ˜ íŒŒë¼ë¯¸í„° : user_message
def get_ai_message(user_message):
    # Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    pc = Pinecone()
    index_name = 'tax-markdown-index'
    index = pc.Index(index_name)
    
    embeddings = OpenAIEmbeddings(model = 'text-embedding-3-large')
    database = PineconeVectorStore(index=index, embedding=embeddings)
    
    # RAG í”„ë¡¬í”„íŠ¸ì™€ LLM ì„¤ì •
    rag_prompt = hub.pull("rlm/rag-prompt")
    llm = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0)
    
    # RetrievalQA ì²´ì¸ ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=database.as_retriever(search_kwargs={"k":4}),
        chain_type_kwargs={"prompt": rag_prompt}
    )

    # query -> ì§ì¥ì¸ -> ê±°ì£¼ìë¡œ ë°”ê¾¸ëŠ” chainì„ ì¶”ê°€
    dictionary = ["ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì"]

    dictionary_prompt = ChatPromptTemplate.from_template(f"""
                    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
                    ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
                    ì‚¬ì „ : {dictionary}
                    ì§ˆë¬¸ : {{question}}
                    """)

    dictionary_chain = dictionary_prompt | llm | StrOutputParser() 

    tax_chain = {"query":dictionary_chain} | qa_chain
    ai_message = tax_chain.invoke({"question": user_message})

    return ai_message

if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})
    
    
    with st.spinner("ë‹µë³€ ìƒì„±ì¤‘ì…ë‹ˆë‹¤."):
        ai_message = get_ai_message(user_question)
        with st.chat_message("ai"):
            st.write(ai_message)
        st.session_state.message_list.append({"role": "ai", "content": ai_message})
        
print(f"after === {st.session_state.message_list}")

